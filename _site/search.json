[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Eli Kravitz, PhD",
    "section": "",
    "text": "Hello!\nI am a statistician and data scientist. I use statistics and machine learning to help federal agencies answer open-ended research questions. This includes sorting through large, messy datasets, benchmarking models, and building data processing pipelines. Before this, I designed clinical trials for the life sciences industry.\nMost of my work involves extensive programming in R and Python. I am a long-time R user who keeps up with latest developments in the language. You can see some of my thoughts on R and Python in the Posts section of this website. I occasionally answer statistics and programming questions on Stack Exchange.\nIn my free time I play music, cook and try (often unsuccessfully) to manage my border collie. You can see me doing the latter two here."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Posts",
    "section": "",
    "text": "Data.table’s (Not Quite) Left Joins\n\n\n\n\n\n\n\nR\n\n\ndata.table\n\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2023\n\n\nEli Kravitz\n\n\n\n\n\n\n  \n\n\n\n\nLLM Aided Webscraping\n\n\nRetreival Augment Generation\n\n\n\n\n\n\n\n\n\nAug 4, 2023\n\n\nEli Kravitz\n\n\n\n\n\n\n  \n\n\n\n\nYemen Analysis Featured by World Bank\n\n\n\n\n\n\n\nnews\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2023\n\n\nEli Kravitz\n\n\n\n\n\n\n  \n\n\n\n\nReading Large Delimited Data in Chunks With Readr\n\n\n\n\n\n\n\nR\n\n\ntidyverse\n\n\nCSV\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2022\n\n\nEli Kravitz\n\n\n\n\n\n\n  \n\n\n\n\nCollaborating With Local Git Repositories\n\n\nLow Tech Data Science\n\n\n\n\nGit\n\n\nlow-tech\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2022\n\n\nEli S. Kravitz\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/yemen/index.html",
    "href": "posts/yemen/index.html",
    "title": "Yemen Analysis Featured by World Bank",
    "section": "",
    "text": "We were asked to analyze the economic and environmental impact of the Yemeni Civil War. I was responsible for analyzing changes in commodity prices during this period of time. In around 2016 the Yemeni economy began to diverge in areas controlled by Yemen’s internationally recognized government (IRG) and areas of de facto authority (DFA). IRG controlled areas faced high inflation, while prices remained relatively stable in DFA control areas.\nOur work was feature on the World Bank’s Blog. A full report is available through the World Bank: Yemen Country Economic Memorandum: Glimmers of Hope in Dark Times. Most of my work is in Section 2, around page 21."
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Dynamic Predictions for the Current Population Survey\n\n\n\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\nEli Kravitz\n\n\n\n\n\n\n  \n\n\n\n\nGraphical Hypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2021\n\n\nEli Kravitz\n\n\n\n\n\n\n  \n\n\n\n\nAdjustments for Patient Crossover in Clinical Trials\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2021\n\n\nEli Kravitz\n\n\n\n\n\n\n  \n\n\n\n\nReevaluating Composite Scores with Flexible Regression and Variable Selection\n\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\nEli Kravitz\n\n\n\n\n\n\n  \n\n\n\n\nEstablishing Physical Activity Guidelines\n\n\nAn Application of Shape Constrained Regression\n\n\n\n\n\n\n\n\n\nJul 1, 2018\n\n\nEli Kravitz\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/data_table_joins/index.html",
    "href": "posts/data_table_joins/index.html",
    "title": "Data.table’s (Not Quite) Left Joins",
    "section": "",
    "text": "Like many other R users, I’ve integrated data.table (CRAN, Github) into my R workflow. data.table is lightning-fast, with common table operations running faster than most R, Python, or standalone alternative. Many people prefer data.table’s concise syntax to tidyverse / dplyr style of pipes and verbs. (I am not one of those people. I mostly use the tidytablepackage which implements tidyversesyntax for data.tables).\nFor as much as I appreciate data.table, it often lacks proper documentation. In particular, the package has no documentation on joins. That leaves the users to learn data.table joins by trial-and-error or through second hand resources and Stack Overflow answers. This fragmented documentation can mask some eccentricities of data.table."
  },
  {
    "objectID": "posts/data_table_joins/index.html#introduction",
    "href": "posts/data_table_joins/index.html#introduction",
    "title": "Data.table’s (Not Quite) Left Joins",
    "section": "",
    "text": "Like many other R users, I’ve integrated data.table (CRAN, Github) into my R workflow. data.table is lightning-fast, with common table operations running faster than most R, Python, or standalone alternative. Many people prefer data.table’s concise syntax to tidyverse / dplyr style of pipes and verbs. (I am not one of those people. I mostly use the tidytablepackage which implements tidyversesyntax for data.tables).\nFor as much as I appreciate data.table, it often lacks proper documentation. In particular, the package has no documentation on joins. That leaves the users to learn data.table joins by trial-and-error or through second hand resources and Stack Overflow answers. This fragmented documentation can mask some eccentricities of data.table."
  },
  {
    "objectID": "posts/data_table_joins/index.html#the-left-join-that-wasnt",
    "href": "posts/data_table_joins/index.html#the-left-join-that-wasnt",
    "title": "Data.table’s (Not Quite) Left Joins",
    "section": "The Left Join That Wasn’t",
    "text": "The Left Join That Wasn’t\nAs of mid 2023 , the first Google search result for “data.table left join” is this Stack Overflow answer and this Stack Overflow answer. Unfortunately, this code does not produce a left join, at least in the traditional SQL sense. Props to Stack Overflow user Helen for pointing this issue. To be fair, this misconception is widespread, so it must be coming from many places.\n\nThe Problem\nThe common advice you’ll get for joining data.tables is to use := operator to update the left table by reference (“in place”) with the new column(s) from the right table. Pseudo-code would look something like this:\n\nLEFT_TABLE[RIGHT_TABLE, on=\"id\", col_from_left := i.col_from_right]\n\nThe problem is data.table does not return multiple matches when you modify by reference. It returns only the last match, instead of returning all matches. See below:\n\nlibrary(data.table)\nA = data.table(id = c(\"x\", \"y\", \"z\"), \n               a_var = c(100, 200, 300))\nB = data.table(id = rep(c(\"w\", \"x\", \"y\", \"z\"), each = 2), \n               b_var = seq(1, 8, by = 1))\n\n# \"Left join\". New values added by reference \nA[B, on = \"id\", new_var := i.b_var]\n\nprint(A)\n\n   id a_var new_var\n1:  x   100       4\n2:  y   200       6\n3:  z   300       8\n\n\nThis returns the wrong number of rows. We should get 6 rows total: each row of A should match two rows in B.\nCompare this with other implementations of left join, and you’ll see the expected behavior.\n\ndplyrBase RSQL\n\n\n\ndplyr::left_join(A, B, by = \"id\")\n\n   id a_var new_var b_var\n1:  x   100       4     3\n2:  x   100       4     4\n3:  y   200       6     5\n4:  y   200       6     6\n5:  z   300       8     7\n6:  z   300       8     8\n\n\n\n\n\nmerge.data.frame(A, B, by = \"id\", all.x =TRUE)\n\n  id a_var new_var b_var\n1  x   100       4     3\n2  x   100       4     4\n3  y   200       6     5\n4  y   200       6     6\n5  z   300       8     7\n6  z   300       8     8\n\n\n\n\n\nsqldf::sqldf(\n  \"SELECT \n    A.id, A.a_var, B.b_var\n  FROM A\n  LEFT JOIN B \n    ON A.id=B.id\n\")\n\ntcltk DLL is linked to '/opt/X11/lib/libX11.6.dylib'\n\n\n  id a_var b_var\n1  x   100     3\n2  x   100     4\n3  y   200     5\n4  y   200     6\n5  z   300     7\n6  z   300     8"
  },
  {
    "objectID": "posts/data_table_joins/index.html#avoiding-the-issue",
    "href": "posts/data_table_joins/index.html#avoiding-the-issue",
    "title": "Data.table’s (Not Quite) Left Joins",
    "section": "Avoiding the issue",
    "text": "Avoiding the issue\nUpdating by value avoids this issue. Of course, you’d need to assign this to a variable.\nYou can use the merge syntax:\n\nmerge.data.table(A, B, by = \"id\", all.x = TRUE)\n\nOr, you can use the X[Y] shorthand without assigning new variables by reference with :=. Confusingly, when using the X[Y] shorthand syntax, you have to reverse the direction of the join.\n\nB[A, on = \"id\"] # Instead of A[B]\n\n   id b_var a_var new_var\n1:  x     3   100       4\n2:  x     4   100       4\n3:  y     5   200       6\n4:  y     6   200       6\n5:  z     7   300       8\n6:  z     8   300       8"
  },
  {
    "objectID": "posts/data_table_joins/index.html#should-i-ever-use-the-other-join",
    "href": "posts/data_table_joins/index.html#should-i-ever-use-the-other-join",
    "title": "Data.table’s (Not Quite) Left Joins",
    "section": "Should I Ever Use the Other Join?",
    "text": "Should I Ever Use the Other Join?\nYes! These “update by reference joins” are really efficient if you know the single match behavior and account for it.\nNormally, R copies the new dataframe in memory after a join. For large datasets, this can be memory intensive and time consuming. You can track this behavior with Hadley Wickham’s lobstr package.\n\n# See where data.tables are stored in memory\nlobstr::obj_addrs(list(A, B))\n\n[1] \"0x7f840dc57a00\" \"0x7f8409d3b200\"\n\njoined_df = merge.data.table(A, B, by = \"id\")\nlobstr::obj_addr(joined_df) # New location in memory\n\n[1] \"0x7f840e910600\"\n\n\nWhen you update-join, the dataframes keep their place in memory. New columns are appended without modifying the objects location in memory.\n\nlobstr::obj_addr(A)\n\n[1] \"0x7f840dc57a00\"\n\nA[B, on = \"id\", foo := 123 * i.b_var]\nlobstr::obj_addr(A) # same location in memory\n\n[1] \"0x7f840dc57a00\""
  },
  {
    "objectID": "posts/csv_chunks/index.html",
    "href": "posts/csv_chunks/index.html",
    "title": "Reading Large Delimited Data in Chunks With Readr",
    "section": "",
    "text": "I recently had to work with several large CSV files, ranging in size from 8Gb to 12Gb. I needed to dplyr::group_by() a categorical variable and count the number of unique records with dplyr::n_distinct(). I ran out of RAM whenever I loaded the entire data set into R with readr::read_csv() or data.table::fread(). I tried to load the data in chunks (see more below), but found the existing documentation to be confusing. I’ve provided a system that works for me in the blog post."
  },
  {
    "objectID": "posts/csv_chunks/index.html#use-case",
    "href": "posts/csv_chunks/index.html#use-case",
    "title": "Reading Large Delimited Data in Chunks With Readr",
    "section": "",
    "text": "I recently had to work with several large CSV files, ranging in size from 8Gb to 12Gb. I needed to dplyr::group_by() a categorical variable and count the number of unique records with dplyr::n_distinct(). I ran out of RAM whenever I loaded the entire data set into R with readr::read_csv() or data.table::fread(). I tried to load the data in chunks (see more below), but found the existing documentation to be confusing. I’ve provided a system that works for me in the blog post."
  },
  {
    "objectID": "posts/csv_chunks/index.html#what-is-chunking",
    "href": "posts/csv_chunks/index.html#what-is-chunking",
    "title": "Reading Large Delimited Data in Chunks With Readr",
    "section": "What is chunking?",
    "text": "What is chunking?\nSometimes you have a dataset that’s too large to fit in memory. One way to get around this is to divide your data into subsets (“chunks”) that do fit into memory and process each chunk separately. You can aggregate the processed chunks together after you’ve reduced the size. This is basically a low-tech implementation of the MapReduce framework used Apache Hadoop"
  },
  {
    "objectID": "posts/csv_chunks/index.html#chunks-in-readr",
    "href": "posts/csv_chunks/index.html#chunks-in-readr",
    "title": "Reading Large Delimited Data in Chunks With Readr",
    "section": "Chunks in readr",
    "text": "Chunks in readr\nYou can read deliminated data in chunks by using any of the read_*_chunked() functions from the readr package. I’ll focus on read_csv_chunked() with a CSV version of mtcars.\nThe function requires at least two arguments:read_csv_chunked(file, callback). file is the file path of of your.csv file. The calllback argument is a little more complicated. The documentation for thisis sparse and aimed at power-users.\nCallbacks tell R what action to take when it’s done reading a chunk. There are three classes of of callbacks that you’re likely to use. Each callback applies a function f() to the chunk before returning a value.\n\nDataFrameCallback - Apply f to each chunk then combine results of f(chunk) by appending rows into a tibble\n\nExample: Read each chunk -&gt; Remove records that don’t meet condition –&gt; append rows\n\nSideEffectChunkCallback - Apply f to reach chunk and return nothing\n\nExample: Reach each chunk –&gt; write each chunk to a .parquet file\n\nAccumulateCallBack - Accumulates a single result across chunks\n\nExample: Count the number of distinct IDs in each chunk –&gt; add them together\n\n\nTo use a callback in read_csv_chunked you declare the function to apply to each chunk then make a new ChunkCallback class. The function must have the arguments data and pos. The data argument holds the current data.frame chunk and pos (short for position) holds the line number that the current chunk begins on. You must include pos in your function arguments even if function does not use it. If you use AccumulateCallBack you must include a third argument, acc, which stores the current value of their accumulator.\nWe’ll use a simple example to show how read a .csv file in chunks with a custom callback function. We load mtcars in chunks and keep cars with manual transmission (am == 0) and miles/gallon over over 20 (mpg &gt;= 20).\n\nlibrary(readr)\nlibrary(dplyr)\ndata(mtcars); write_csv(mtcars, \"mtcars.csv\")\n\n# Function to pass to DataFrameCallback$new()\nf = function(data, pos) {  # pos must be an  argument even though unused\n  data %&gt;% \n    filter(am == 0L, mpg &gt;= 20)\n}\n\nchunked_df = read_csv_chunked(\n  file = \"mtcars.csv\",\n  callback = DataFrameCallback$new(f),\n  chunk_size = 5L\n)\n\nchunked_df\n\n# A tibble: 4 × 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n2  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n3  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n4  21.5     4  120.    97  3.7   2.46  20.0     1     0     3     1"
  },
  {
    "objectID": "posts/csv_chunks/index.html#why-didnt-you-use-xyz",
    "href": "posts/csv_chunks/index.html#why-didnt-you-use-xyz",
    "title": "Reading Large Delimited Data in Chunks With Readr",
    "section": "Why didn’t you use XYZ?",
    "text": "Why didn’t you use XYZ?\nWhy didn’t you use XYZ?\nYou can use distributed data processing like Spark or Dask. I didn’t have access to either.\nThere are options for reading chunked data inside and outside of R.\n\nPython’s pandas.read_csv(..., chunksize) returns an iterator to read a read a CSV in chunks. This is a good option if you’re okay leaving the R and tidyverse ecosystem.\nfread(...,) from data.table can read files in chunks using the skip and nrows arguments. However, the user has to manually program this functionality. Things get complicated if you want to keep column names or apply a function to each chunk.\nR’s LaF package offers fast, random access to ASCII files without loading the file into memory. read_chunkwise() from the chunked package is a wrapper that provides dplyr like synatx to Laf. I couldn’t get either package to work, and I couldn’t diagnose the problem from the error messages."
  },
  {
    "objectID": "posts/local_git/index.html",
    "href": "posts/local_git/index.html",
    "title": "Collaborating With Local Git Repositories",
    "section": "",
    "text": "I often work with restricted data that has to be isolated on its own network. That means no access to Github for version control. I set up a Git repository that’s stored locally. It lets me and my collaborators work on version control without having internet access. I couldn’t find many resources for setting this up, so I decided to put this post together.\nThis is also a tutorial for collaborators who are new to Git; some terminology is simplified and some steps are explain in more detail than is strictly necessary."
  },
  {
    "objectID": "posts/local_git/index.html#background",
    "href": "posts/local_git/index.html#background",
    "title": "Collaborating With Local Git Repositories",
    "section": "",
    "text": "I often work with restricted data that has to be isolated on its own network. That means no access to Github for version control. I set up a Git repository that’s stored locally. It lets me and my collaborators work on version control without having internet access. I couldn’t find many resources for setting this up, so I decided to put this post together.\nThis is also a tutorial for collaborators who are new to Git; some terminology is simplified and some steps are explain in more detail than is strictly necessary."
  },
  {
    "objectID": "posts/local_git/index.html#preliminaries",
    "href": "posts/local_git/index.html#preliminaries",
    "title": "Collaborating With Local Git Repositories",
    "section": "Preliminaries",
    "text": "Preliminaries\nThis assumes some basic familiarity with Git and Github. If you don’t have this background, check out Happy Git and GitHub for the userR by Jennifer Bryan for an R focused introduction to Git or GitHowTo for a language agnostic introduction.\nYou need to set a user name and email address for Git. These are displayed when you commit changes; they will not be connected to any of your actual accounts.\nEnter the following in the console.\n\ngit config --global user.name \"Hugh Jazz\" # Replace with your name\ngit config --global user.email \"hugh.jazz@hotmail.com\" # Replace with your email\n\nYou can check your config settings with git config -l or git config --list.\n\ngit config --l\n\n\n## user.name=\"Hugh Jazz\"\n## user.email=\"hugh.jazz@hotmail.gov'\n\nI recommend adding the following command to your git config. This command allows you to type git hist to see a cleanly formatted history of your repository.\n\ngit config --global alias.hist \"log --pretty=format:'%h %ad | %s%d [%an]' --graph --date=short\""
  },
  {
    "objectID": "posts/local_git/index.html#setting-up-a-local-remote-repository",
    "href": "posts/local_git/index.html#setting-up-a-local-remote-repository",
    "title": "Collaborating With Local Git Repositories",
    "section": "Setting up a Local Remote Repository",
    "text": "Setting up a Local Remote Repository\nInstead of Github hosting our remote repository, we’ll place our remote in a directory on shared network drive that all project members have access to. Git. We need to make a bare repository. A bare repository contains only the information Git needs to create the files in the repository and their history. It does not directly contain the files you are editing (the “Working Tree”).\nThis guide sets up a repository /user/example_project.git to illustrate the steps. Note that example_project.git is a directory, not a .git file. It’s customary to give bare repos a .git extension.\nInitiate the bare repository:\n\nmkdir /user/example_project.git  # Bare repos are customarily given a .git extension\ncd /user/example_project.git\ngit init --bare \n\n\n## Initialized empty Git repository in /user/your_directory/example_project.git\n\nUsers cannot work in bare repositories directly. You need to clone the repo and push changes.\nI clone clone the repo, add some basic structure to the project, then push the changes to remote. You should (but aren’t required) to give the local repo the same name as the remote. For illustration, I append _local to the end of my local repo to distinguish it from the remote.\nMake a git repo:\n\nmkdir /user/eli/some/file/path/example_project_local # Make empty directory\ncd  /user/eli/some/file/path/example_project_local\ngit init\ngit remote add origin /user/example_project.git  # connect the remote\n\nVerify that you set the remote correctly\n\ngit remote -v\n\n\n## origin /user/example_project.git (fetch)\n## origin /user/example_project.git (push)\n\nI add a README.md file, some folders, and dummy files to my local repository. You can do this in Dolphin or from the console. Nothing is specifically required in this step. Make sure to open RStudio and assosciate this directory with a project so you can use RStudio’s Git GUI later. Do not select the “Create a git repository” checkbox. You’ll know if you set up a RStudio project if you see a .Rproj file in your directory.\nMy repo looks like this:\n.git\\\nR\\\ndummy_file.R\ndata\\\ndocumentation\\\nimportant_text_file.txt\n.gitignore\nexample_repo_eli.Rproj\nREADME.md\nI edit the .gitignore file to not track csv or SAS files. These files are usually large and should be stored in proejcts/data. Important data files can be manually added with git commit --force some_data_file.csv.\nMy .gitignore has the following lines.\n.Rproj.user\n.Rhistory\n.Rdata\n*.csv\n*sas7bdat\n*.sas7bcat\n*.xpt\nThe first commit and push need to be done from console. After that you can use RStudio or continue using the console.\n\ngit add *\ngit add .gitignore\ngit commit -am \"Initial commit\" \ngit push -u origin master"
  },
  {
    "objectID": "posts/local_git/index.html#setup-for-remaining-users",
    "href": "posts/local_git/index.html#setup-for-remaining-users",
    "title": "Collaborating With Local Git Repositories",
    "section": "Setup for Remaining Users",
    "text": "Setup for Remaining Users\nFirst time users need to use console to clone a local copy of the remote. After that, users can use RStudio’s Git GUI to stage, commit, push, and pull from the local repository. RStudio will track all files, not just .R, .Rds, and other R files. Users may also continue to use Git from the console.\n\ncd /projects/users/YOUR_USERNAME/\ngit clone /projects/programs/sipp_cps_project.git # Puts repo into a folder called example_repo"
  },
  {
    "objectID": "posts/local_git/index.html#typical-workflow",
    "href": "posts/local_git/index.html#typical-workflow",
    "title": "Collaborating With Local Git Repositories",
    "section": "Typical Workflow",
    "text": "Typical Workflow\nAfter the initial setup, the workflow is the same as using Github. You can commit, push, pull, revert changes, and make new branches from RStudio or the console. Instructions for using Git in Rstudio are available in Section 4.9 of Introduction to Computational and Data Science. Example console input is provided below:\n\ngit pull # If you need to update you branch from remote\n\ngit add * # Add your files \ngit commit -am \"My commit message\" \ngit push \n\nThe workflow is the same as if you were using Git with Github:\n\nPull from the remote repository to sync changes to your local repository\nStage files to indicate what you want Git to track\nCommit your local changes to take a “snapshot” of the work you staged in the previous step\nPush your changes to remote to include your local changes in the “global” repository"
  },
  {
    "objectID": "presentations/aapor/index.html",
    "href": "presentations/aapor/index.html",
    "title": "Dynamic Predictions for the Current Population Survey",
    "section": "",
    "text": "In May of 2023, I gave a presentation at the American Association for Public Opinion Research (AAPOR) conference. This presentation was given to an audience of, mostly, social scientists, so I omitted some of the technical details.\nOur goal is to predict who will respond to the Current Population Survey (CPS). Survey interviews happen over 10 days. To account for time varying effects, we stratifiy our data by interview day and fit an XGBoost model for each day.\nLink to presentation"
  },
  {
    "objectID": "presentations/os_crossover/index.html",
    "href": "presentations/os_crossover/index.html",
    "title": "Adjustments for Patient Crossover in Clinical Trials",
    "section": "",
    "text": "This presentation was part of an information session and workshop for Oncology statisticians. I review methods to adjust estimates of treatment effect when clinical trial patients can crossover from the standard-of-care arm to the treatment arm. I provide references so readers can review topic relevant to them at a later time.\nLink to presentation"
  },
  {
    "objectID": "presentations/graphical_testing/index.html",
    "href": "presentations/graphical_testing/index.html",
    "title": "Graphical Hypothesis Testing",
    "section": "",
    "text": "This presentation began as a proposed design for a Phase 2 trial. After Phase 1 results, development was stopped and this trial was never considered. I reworked the presentation into a tutorial/workshop for using graphical testing in a multi-arm trial. This was my first experiment with plotly and Rmarkdown slides, so excuse any strange formatting.\nLink to presentation"
  },
  {
    "objectID": "presentations/pa_presentation/index.html",
    "href": "presentations/pa_presentation/index.html",
    "title": "Establishing Physical Activity Guidelines",
    "section": "",
    "text": "This is an old presentation. When I was getting ready to finish my PhD, this is what I presented whenever I was asked to give a talk during an interview.\nWe developed physical activity guidelines by fitting shaped constrained regression models to self-reported physical activity questionnaires. We scaled the model’s fitted values so they were between 0 and 100, with 0 indicate extremely poor levels of physical activity and 100 representation perfect levels of physical activity.\nLink to the presentation"
  },
  {
    "objectID": "presentations/hei/index.html",
    "href": "presentations/hei/index.html",
    "title": "Reevaluating Composite Scores with Flexible Regression and Variable Selection",
    "section": "",
    "text": "I planned to give this presentation (or a variation of it) at International Conference on Diet and Activity Methods (ICDAM) in the spring of 2020. Certain global events prevented me from traveling in 2020, so I never had a chance to give this presentation.\nThe Healthy Eating Index (HEI) is a set of guidelines for what types of foods Americans should consume. A perfect diet receives a score of 100 and a poor diet receives a score of 0.\nThis presentation shows how you can use a Lasso penalty to adjust the relative importance of different good groups. The Lasso penalty allows the relative importance to go to 0, suggesting that food group should not be consumed.\nLink to presentation"
  },
  {
    "objectID": "posts/web_scraper/index.html",
    "href": "posts/web_scraper/index.html",
    "title": "LLM Aided Webscraping",
    "section": "",
    "text": "Small Business Development Centers (SBDCs) provide counseling and resources to entrepreneurs and small business owners. SBDCs are typically funded by federal, state, and local governments and partnerships with educational institutions, economic development agencies, and private organizations. The primary goal of SBDCs\nis to support the growth and development of small businesses in their communities.\nWe were ask to compile a comprehensive list of SBDCs. Additionally, we were required to outline the specific services offered by each SBDC (such as business counseling, tax assistance, etc.) and provide details about the staff ar each center."
  },
  {
    "objectID": "posts/web_scraper/index.html#introduction",
    "href": "posts/web_scraper/index.html#introduction",
    "title": "LLM Aided Webscraping",
    "section": "",
    "text": "Small Business Development Centers (SBDCs) provide counseling and resources to entrepreneurs and small business owners. SBDCs are typically funded by federal, state, and local governments and partnerships with educational institutions, economic development agencies, and private organizations. The primary goal of SBDCs\nis to support the growth and development of small businesses in their communities.\nWe were ask to compile a comprehensive list of SBDCs. Additionally, we were required to outline the specific services offered by each SBDC (such as business counseling, tax assistance, etc.) and provide details about the staff ar each center."
  },
  {
    "objectID": "posts/web_scraper/index.html#the-pivot-to-ai",
    "href": "posts/web_scraper/index.html#the-pivot-to-ai",
    "title": "LLM Aided Webscraping",
    "section": "The Pivot to AI",
    "text": "The Pivot to AI\nFinding a list of all SBDCs was straightforward; they can be scraped from the Find Your SBDC tool provided by America’s SBDC. We found over 700 SBDCs using this tool, each with their own unique website. Manually going through each page is not feasible. Our original plan to extract relevant information with XPath or CSS selectors was not possible. SBDC websites are not standardized. Building selectors that generally work over such a wide variety of websites is impossible.\nFor each of the 700+ SBDC websites we use Python’s Beautiful Soup library to retrieve the text from every subpage of the SBDC’s website. Ideally, we would provide all this text to ChatGPT API as context in a prompt then ask ChatGPT to retrieve the staff and services for us. GPT-4 has a 8194 token limit (≈3000 words) which includes the response from GPT-4. We limit our context to 3000 tokens to leave tokens remaining for our prompt and answer.\nThis approach is adapted from webcrawler notebook from the OpenAI Cookbook."
  },
  {
    "objectID": "posts/web_scraper/index.html#retrieval-augmented-generation",
    "href": "posts/web_scraper/index.html#retrieval-augmented-generation",
    "title": "LLM Aided Webscraping",
    "section": "Retrieval-Augmented Generation",
    "text": "Retrieval-Augmented Generation\nWe take the following steps to implement retrieval-augmented generation. Each step is explained in detail in later sections of this document.\nAs an example, I’m showing text scraped from the SBDC website for Tyler, TX and every sub-page linked from the main URL. Results are stored in a Pandas DataFrame.\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"processed/scraped.csv\", index_col=None, header=0)\n\n\n\n\n\n  \n\n\n\nYou can see text scraped from one page in more detailed below.\n\n\nCode\n# example_text = \"Business Advice | Tyler Small Business Development Center Follow Us x a ! 903-510-2975 Directions Business Advising The SBDC offers free one-on-one advising for start-up and existing businesses in the retail, wholesale, manufacturing, and service industries. Our Advisors can help you. Prepare financial projections for loan package. Conduct financial analysis and improve profitability Develop competitive strategies  Cope with government regulation and taxes Design a marketing plan Understand and secure loans and other sources of capital Keep abreast of current technology Brainstorming and research Restructure debt analysis Prepackage Interview Process for SBA Loans Turnaround Advising Follow these steps to meet with a business advisor Step 1 Fill out/ download the following forms. Start-up Business Assessment Existing Business Assessment Fill out the Form 641/Client Agreement Step 2: You will be contacted by our office to direct you to the appropriate resource.  How to Start a Business and Market lt on Social Media Tyler SBDC offers a low-cost monthly class for individuals interested in starting a small business. This single-session class provides an excellent opportunity for individuals to determine if small business ownership is right for them. This class also covers current social media trends and how to use platforms such as Instagram, Facebook, YouTube, and more to market their new business. This class is held on Tuesdays as follows: Tuesday 6/6 6-8 pm Tuesday 7/11 6-8 pm Tuesday 8/1 6-8 pm Location: RTDC 112 Registration Fee: $25 Call TJC School of Continuing Studies at 903-510-2900 to register and pay Contact Us\"\n\n\n\nEmbedding\n\nEstimating Cost and Resources\nFirst, we estimate how much of our 8194 token limit will be used by this text using the tiktoken library (Github). tiktoken is a open-source byte pair encoding (BPE) tokenizer. You can use this approximate how many tokens you’ll send to the Chat-GPT API. The example text uses about 329 tokens.\n\nimport tiktoken \ntokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\ndf['n_tokens'] = df['text'].apply(lambda x: len(tokenizer.encode(x)))\n\n\n\n\n\n  \n\n\n\n\n\n\nEmbedding Text\nNote: You’ll need to make an access key for OpenAI to run any of the code in this section\nThen we create embeddings for each page using the OpenAI API. Embeddings translate text into numeric vectors. When two vector embeddings are close to each other (low distance), we expect the text to be similar in meaning. OpenAI offers a number of embedding models for users to choose from. At the time of writing this, text-embedding-ada-002 is is the cheapest option and outperforms the other models.\nBelow is example code (not run), to show how to use OpenAI embedding models. (I didn’t want to pay for any more uses of the OpenAI API or host a reproducible example on this site. )\n\nimport tiktoken \nfrom openai import Embedding\n\ndf[\"embeddings\"] = df[\"text\"].apply(lambda x: Embedding.create(input=x,model=\"text-embedding-ada-002\",temperature=0.25)[\"data\"][0][\"embedding\"])"
  },
  {
    "objectID": "posts/web_scraper/index.html#informational-retrieval",
    "href": "posts/web_scraper/index.html#informational-retrieval",
    "title": "LLM Aided Webscraping",
    "section": "Informational Retrieval",
    "text": "Informational Retrieval\nWe’ll use the following query:\n\nquery = f\"Provide a list of help, resources, or services offered by {sbdc_name}. Each list item should begin with a dash and end with a newline. Summarize each list item into at most 2 to 4 words.\"\n\nWe embed the query then compare the distance between it and each webpage’s text. We rank the text by most relevant to our query to least relevant.\nExample code:\n\nfrom numpy import dot\nfrom numpy.linalg import norm\n\ndef cos_dist(a, b):\n  1 - dot(a, b)/(norm(a) * norm(b))\n  \nq_embedding = Embedding.create(input=question, engine=\"text-embedding-ada-002\")[\"data\"][0][\"embedding\"]\n\n# Get the distance between the query and each embedded website text\ndf[\"distances\"] = [cos_dist(q_embedding, embedding) for embedding in df[\"embeddings\"]]\n  \n# Smallest distance means more relevant\ndf[\"distances\"] = df.sort_values(\"distances\", ascending=True)\n\nThis is an example of what we would see at this point.\n\n\n\n\n\n\n\n\n\n\nRelevance\nWebsite\ndistancse\nn_tokens\nCumulative Tokens Used\n\n\n\n\n1\nResources\n0.158\n459\n459\n\n\n2\nBusiness Advice\n0.16\n358\n817\n\n\n3\nAbout SBDC\n0.173\n380\n1197\n\n\n4\nAbout Us\n0.173\n381\n1578\n\n\n5\nStories\n0.18\n342\n1920\n\n\n6\nWho We Are\n0.189\n652\n2572\n\n\n7\nEvents\n0.194\n577\n3149\n\n\n\nWe append the most relevant text until we hit our GPT-4 API character limit\n\ncontext = []\ncur_len = 0\nmax_len = 3000\n\n# Sort by distance and add the text to the context until the context is too long\nfor i, row in df.sort_values(\"distances\", ascending=True).iterrows():\n  # Add the length of the text to the current length plus some padding for separators\n  cur_len += row[\"n_tokens\"] + 4\n  \n  # If the context is too long, break\n  if cur_len &gt; max_len:\n    break\n  \n  # Else add it to the text that is being returned\n  context.append(row[\"text\"])\n  \n  # Return the context\n  return \"\\n\\n###\\n\\n\".join(context)"
  },
  {
    "objectID": "posts/web_scraper/index.html#prompting-gpt-4-with-context",
    "href": "posts/web_scraper/index.html#prompting-gpt-4-with-context",
    "title": "LLM Aided Webscraping",
    "section": "Prompting GPT-4 With Context",
    "text": "Prompting GPT-4 With Context\nFinally, we submit our query to GPT-4 along with the context we’ve found in the previous steps.\n\nresponse = openai.ChatCompletion.create(\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a web scraping assistant. Answer the user's questions based\"\n                    + \"on the context below. If the question can't be answered based on the context,\"\n                    + f'say \"I don\\'t know\"\\n\\nContext: {context}',\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": question_context.question,\n                },\n            ],\n            temperature=0.15,\n            top_p=1,\n            frequency_penalty=0,\n            presence_penalty=0,\n            stop=stop_sequence,\n            model=model,\n        )\nresponse[\"choices\"][0][\"message\"][\"content\"]\n\n\n\n\n- Business Advising\n- Business Classes\n- Start-up Assistance\n- Existing Business Development\n- Training Programs\n- Research Capabilities\n- Business Plan Preparation\n- SBA Loan Paperwork Preparation\n- Practical Business Assistance\n- Mailing List with Resources and Information\n- Start-up Business Assessment\n- Existing Business Assessment"
  },
  {
    "objectID": "posts/web_scraper/index.html#webscraping",
    "href": "posts/web_scraper/index.html#webscraping",
    "title": "LLM Aided Webscraping",
    "section": "Webscraping",
    "text": "Webscraping\nAs an example, I’m showing text scraped from the SBDC website for Tyler, TX and every sub-page of the main URL. Results are stored in a pandas.DataFrame.\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"processed/scraped.csv\", index_col=None, header=0)\n\n\n\n\n\n  \n\n\n\nYou can see text scraped from one page in more detail by expanding the code chunk below:\n\n\nCode\n# example_text = \"Business Advice | Tyler Small Business Development Center Follow Us x a ! 903-510-2975 Directions Business Advising The SBDC offers free one-on-one advising for start-up and existing businesses in the retail, wholesale, manufacturing, and service industries. Our Advisors can help you. Prepare financial projections for loan package. Conduct financial analysis and improve profitability Develop competitive strategies  Cope with government regulation and taxes Design a marketing plan Understand and secure loans and other sources of capital Keep abreast of current technology Brainstorming and research Restructure debt analysis Prepackage Interview Process for SBA Loans Turnaround Advising Follow these steps to meet with a business advisor Step 1 Fill out/ download the following forms. Start-up Business Assessment Existing Business Assessment Fill out the Form 641/Client Agreement Step 2: You will be contacted by our office to direct you to the appropriate resource.  How to Start a Business and Market lt on Social Media Tyler SBDC offers a low-cost monthly class for individuals interested in starting a small business. This single-session class provides an excellent opportunity for individuals to determine if small business ownership is right for them. This class also covers current social media trends and how to use platforms such as Instagram, Facebook, YouTube, and more to market their new business. This class is held on Tuesdays as follows: Tuesday 6/6 6-8 pm Tuesday 7/11 6-8 pm Tuesday 8/1 6-8 pm Location: RTDC 112 Registration Fee: $25 Call TJC School of Continuing Studies at 903-510-2900 to register and pay Contact Us\"\n\n\n\nEmbedding\n\nEstimating Cost and Resources\nFirst, we estimate how much of our 8194 token limit will be used by this text using the tiktoken library (Github). tiktoken is a open-source byte pair encoding (BPE) tokenizer. You can use this approximate how many tokens you’ll send to the Chat-GPT API. The example text uses about 329 tokens.\n\nimport tiktoken \ntokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\ndf['n_tokens'] = df['text'].apply(lambda x: len(tokenizer.encode(x)))\n\n\n\n\n\n  \n\n\n\n\n\n\nEmbedding Text\nNote: You’ll need to make an access key for OpenAI to run any of the code in this section\nThen we create embeddings for each page using the OpenAI API. Embeddings translate text into numeric vectors. When two vector embeddings are close to each other (low distance), we expect the text to be similar in meaning. OpenAI offers a number of embedding models for users to choose from. At the time of writing this, text-embedding-ada-002 is is the cheapest option and outperforms the other models.\nBelow is example code (not run), to show how to use OpenAI embedding models. (I didn’t want to pay for any more uses of the OpenAI API or host a reproducible example on this site. )\n\nimport tiktoken \nfrom openai import Embedding\n\ndf[\"embeddings\"] = df[\"text\"].apply(lambda x: Embedding.create(input=x,model=\"text-embedding-ada-002\",temperature=0.25)[\"data\"][0][\"embedding\"])"
  }
]