{
  "hash": "d7ba4dd401a1f04b87753b5ea64606f7",
  "result": {
    "markdown": "---\ntitle: \"Reading Large Delimited Data in Chunks With Readr\"\nauthor: \"Eli Kravitz\"\ndate: \"2022-04-01\"\ncategories: [R, tidyverse, CSV]\nimage: \"chunk.jpg\"\nformat: \n  html:\n    df-print: default\n---\n\n\n## Use Case\n\nI recently had to work with several large CSV files, ranging in size from 8Gb to 12Gb. I needed to `dplyr::group_by()` a categorical variable and count the number of unique records with `dplyr::n_distinct()`. I ran out of RAM whenever I loaded the entire data set into R with `readr::read_csv()` or `data.table::fread()`. I tried to load the data in chunks (see more below), but found the existing documentation to be confusing. I've provided a system that works for me in the blog post.\n\n## What is chunking?\n\nSometimes you have a dataset that's too large to fit in memory. One way to get around this is to divide your data into subsets (\"chunks\") that do fit into memory and process each chunk separately. You can aggregate the processed chunks together after you've reduced the size. This is basically a low-tech implementation of the [MapReduce](https://en.wikipedia.org/wiki/MapReduce#Overview) framework used [Apache Hadoop](https://www.ibm.com/cloud/blog/hadoop-vs-spark)\n\n## Chunks in readr\n\nYou can read deliminated data in chunks by using any of the [`read_*_chunked()` functions](https://readr.tidyverse.org/reference/read_delim_chunked.html) from the [`readr` package](https://readr.tidyverse.org/index.html). I'll focus on `read_csv_chunked()` with a CSV version of `mtcars`.\n\nThe function requires at least two arguments:`read_csv_chunked(file, callback)`. `file` is the file path of of your`.csv` file. The `calllback` argument is a little more complicated. The [documentation](https://readr.tidyverse.org/reference/callback.html#ref-examples) for thisis sparse and [aimed at power-users](https://github.com/tidyverse/readr/issues/510#issuecomment-242363754).\n\nCallbacks tell R what action to take when it's done reading a chunk. There are three classes of of callbacks that you're likely to use. Each callback applies a function `f()` to the chunk before returning a value.\n\n1.  `DataFrameCallback` - Apply `f` to each chunk then combine results of `f(chunk)` by appending rows into a `tibble`\n    -   Example: Read each chunk -\\> Remove records that don't meet condition --\\> append rows\n2.  `SideEffectChunkCallback` - Apply `f` to reach chunk and return nothing\n    -   Example: Reach each chunk --\\> write each chunk to a `.parquet` file\n3.  `AccumulateCallBack` - Accumulates a single result across chunks\n    -   Example: Count the number of distinct IDs in each chunk --\\> add them together\n\nTo use a callback in `read_csv_chunked` you declare the function to apply to each chunk then make a new `ChunkCallback` class. The function must have the arguments `data` and `pos`. The `data` argument holds the current data.frame chunk and `pos` (short for position) holds the line number that the current chunk begins on. **You must include `pos` in your function arguments even if function does not use it.** If you use `AccumulateCallBack` you must include a third argument, `acc`, which stores the current value of their accumulator.\n\nWe'll use a simple example to show how read a `.csv` file in chunks with a custom callback function. We load `mtcars` in chunks and keep cars with manual transmission (`am == 0`) and miles/gallon over over 20 (`mpg >= 20`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nlibrary(dplyr)\ndata(mtcars); write_csv(mtcars, \"mtcars.csv\")\n\n# Function to pass to DataFrameCallback$new()\nf = function(data, pos) {  # pos must be an  argument even though unused\n  data %>% \n    filter(am == 0L, mpg >= 20)\n}\n\nchunked_df = read_csv_chunked(\n  file = \"mtcars.csv\",\n  callback = DataFrameCallback$new(f),\n  chunk_size = 5L\n)\n\nchunked_df\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 Ã— 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n2  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n3  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n4  21.5     4  120.    97  3.7   2.46  20.0     1     0     3     1\n```\n:::\n:::\n\n\n## Why didn't you use XYZ?\n\n*Why didn't **you** use XYZ?*\n\nYou can use distributed data processing like [Spark](https://spark.apache.org/docs/latest/sql-data-sources-csv.html) or [Dask](https://www.coiled.io/blog/dask-read-csv-to-dataframe). I didn't have access to either.\n\nThere are options for reading chunked data inside and outside of R.\n\n-   Python's [`pandas.read_csv(..., chunksize)`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) returns an iterator to read a read a CSV in chunks. This is a good option if you're okay leaving the R and tidyverse ecosystem.\n-   `fread(...,)` from `data.table` can read files in chunks using the `skip` and `nrows` arguments. However, the user has to manually [program this functionality](https://stackoverflow.com/a/60085589/2838936). Things get complicated if you want to keep column names or apply a function to each chunk.\n-   R's [`LaF` package](https://cran.r-project.org/web/packages/LaF/index.html) offers fast, random access to ASCII files without loading the file into memory. `read_chunkwise()` from the [`chunked` package](https://cran.r-project.org/web/packages/chunked/index.html) is a wrapper that provides `dplyr` like synatx to `Laf`. I couldn't get either package to work, and I couldn't diagnose the problem from the error messages.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}